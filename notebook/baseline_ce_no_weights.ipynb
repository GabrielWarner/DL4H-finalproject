{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielWarner/DL4H-finalproject/blob/main/notebook/baseline_ce_no_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q \"transformers==4.44.2\" \"datasets>=2.20.0\" \"evaluate==0.4.2\" pandas matplotlib tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "dNnYq_DMdARr",
        "outputId": "47c4077f-d77b-4ed2-b558-8c97b143864a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PackagePath' object has no attribute '_drv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2900820353.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install -q --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install -q \"transformers==4.44.2\" \"datasets>=2.20.0\" \"evaluate==0.4.2\" pandas matplotlib tqdm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[0;32m--> 958\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     }\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mparts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \"\"\"An object providing sequence-like access to the\n\u001b[1;32m    705\u001b[0m         components in the filesystem path.\"\"\"\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_parse_path\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# e.g. //?/unc/server/share\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR  = \"/content/drive/MyDrive/DL4H_data/mimic\"\n",
        "CKPT_DIR  = \"/content/drive/MyDrive/DL4H_data/ckpt\"\n",
        "LOGS_DIR  = \"/content/drive/MyDrive/DL4H_data/logs\"\n",
        "FIGS_DIR  = \"/content/drive/MyDrive/DL4H_data/figs\"\n",
        "\n",
        "import os\n",
        "for p in [DATA_DIR, CKPT_DIR, LOGS_DIR, FIGS_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)"
      ],
      "metadata": {
        "id": "-U4gCT-EdBmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/DL4H_data/hf_cache\"\n",
        "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)"
      ],
      "metadata": {
        "id": "QAOkPE2CdFK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ds = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\")\n",
        "print(ds)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"report_id\": np.arange(len(ds[\"train\"])),\n",
        "    \"findings\": ds[\"train\"][\"findings\"],\n",
        "    \"impression\": ds[\"train\"][\"impression\"],\n",
        "})\n",
        "df[\"report_text\"] = (\n",
        "    df[\"impression\"].fillna(\"\").astype(str).str.strip() + \" \" +\n",
        "    df[\"findings\"].fillna(\"\").astype(str).str.strip()\n",
        ").str.strip()\n",
        "\n",
        "# drop empty reports\n",
        "df = df[df[\"report_text\"].str.len() > 0].reset_index(drop=True)\n",
        "print(\"Reports after filtering:\", len(df))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "RHaelCH0dGyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(42)\n",
        "perm = rng.permutation(len(df))\n",
        "n = len(df)\n",
        "i_tr = int(0.8*n); i_va = int(0.9*n)\n",
        "\n",
        "df[\"split\"] = \"test\"\n",
        "df.loc[perm[:i_tr], \"split\"] = \"train\"\n",
        "df.loc[perm[i_tr:i_va], \"split\"] = \"val\"\n",
        "\n",
        "df[\"split\"].value_counts()"
      ],
      "metadata": {
        "id": "xkfRG4fZdMaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def sent_tokenize(text: str):\n",
        "    parts = re.split(r'(?<=[\\.\\?\\!])\\s+|\\n+', text)\n",
        "    return [s.strip() for s in parts if s and s.strip()]\n",
        "\n",
        "ABN_TERMS = [\n",
        "    \"pneumonia\",\"consolidation\",\"edema\",\"effusion\",\"atelectasis\",\"pneumothorax\",\n",
        "    \"fracture\",\"opacity\",\"lesion\",\"mass\",\"enlarged\",\"cardiomegaly\",\"infiltrate\",\n",
        "    \"hemorrhage\",\"emphysema\",\"fibrosis\",\"collapse\",\"airspace\",\"air-fluid\",\"pleural\",\n",
        "    \"mediastinal widening\",\"hyperinflation\",\"interstitial\",\"ground-glass\"\n",
        "]\n",
        "NORM_PHRASES = [\n",
        "    \"no acute cardiopulmonary process\",\"no acute cardiopulmonary disease\",\n",
        "    \"no acute process\",\"no acute abnormality\",\"no acute findings\",\"no focal consolidation\",\n",
        "    \"no pleural effusion\",\"no pneumothorax\",\"heart size is normal\",\"lungs are clear\",\n",
        "    \"no acute osseous abnormality\"\n",
        "]\n",
        "UNCERTAIN_MARKERS = [\n",
        "    \"cannot exclude\",\"question of\",\"possible\",\"may represent\",\"suggest\",\"probable\",\n",
        "    \"likely\",\"suspicious for\",\" ?\",\" ? \"\n",
        "]\n",
        "\n",
        "abn_re  = re.compile(r\"\\b(\" + \"|\".join(re.escape(w) for w in ABN_TERMS) + r\")\\b\", re.I)\n",
        "norm_re = re.compile(\"|\".join(re.escape(p) for p in NORM_PHRASES), re.I)\n",
        "unc_re  = re.compile(\"|\".join(re.escape(p) for p in UNCERTAIN_MARKERS), re.I)\n",
        "\n",
        "def weak_label(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        return \"uncertain\"\n",
        "    has_abn  = bool(abn_re.search(s))\n",
        "    has_norm = bool(norm_re.search(s))\n",
        "    has_unc  = bool(unc_re.search(s))\n",
        "    if has_abn: return \"abnormal\"\n",
        "    if has_norm and not has_abn: return \"normal\"\n",
        "    return \"uncertain\""
      ],
      "metadata": {
        "id": "iMxKvCi5dO0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for idx, r in tqdm(df.iterrows(), total=len(df)):\n",
        "    sents = sent_tokenize(r[\"report_text\"])\n",
        "    for j, sent in enumerate(sents):\n",
        "        if len(sent) < 3:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"report_id\": int(r[\"report_id\"]),\n",
        "            \"sentence_id\": j,\n",
        "            \"text\": sent,\n",
        "            \"label\": weak_label(sent),\n",
        "            \"split\": r[\"split\"],\n",
        "        })\n",
        "\n",
        "sent_df = pd.DataFrame(rows)\n",
        "print(\"Total sentences:\", len(sent_df))\n",
        "sent_df.head(5)"
      ],
      "metadata": {
        "id": "WtdqKk1cdR5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dist(df):\n",
        "    return df[\"label\"].value_counts(normalize=True).round(3).to_dict()\n",
        "\n",
        "print(\"ALL:\", dist(sent_df))\n",
        "for sp in [\"train\",\"val\",\"test\"]:\n",
        "    print(sp, dist(sent_df[sent_df[\"split\"]==sp]))"
      ],
      "metadata": {
        "id": "JMDexTv0dVyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    out = (sent_df[sent_df[\"split\"]==split]\n",
        "           [[\"report_id\",\"sentence_id\",\"text\",\"label\"]]\n",
        "           .reset_index(drop=True))\n",
        "    out_path = f\"{DATA_DIR}/{split}.csv\"\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(split, len(out), \"->\", out_path)\n",
        "\n",
        "import pandas as pd\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    path = f\"{DATA_DIR}/{split}.csv\"\n",
        "    df_split = pd.read_csv(path)\n",
        "    df_split.sample(n=min(1000, len(df_split)), random_state=1).to_csv(\n",
        "        f\"{DATA_DIR}/{split}_mini.csv\", index=False\n",
        "    )\n",
        "print(\"Wrote mini splits.\")"
      ],
      "metadata": {
        "id": "BZazqkp6dYBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def label_dist(path):\n",
        "    df = pd.read_csv(path)\n",
        "    return {\"rows\": len(df), \"dist\": df[\"label\"].value_counts(normalize=True).round(3).to_dict()}\n",
        "\n",
        "print({\n",
        "    \"source\": \"HF itsanmolgupta/mimic-cxr-dataset + weak sentence labels (rule-based)\",\n",
        "    \"train\": label_dist(f\"{DATA_DIR}/train.csv\"),\n",
        "    \"val\":   label_dist(f\"{DATA_DIR}/val.csv\"),\n",
        "    \"test\":  label_dist(f\"{DATA_DIR}/test.csv\"),\n",
        "})"
      ],
      "metadata": {
        "id": "CGyhSmQGRlLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, platform\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "import transformers, datasets, evaluate\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"evaluate:\", evaluate.__version__)\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"CSV sizes:\",\n",
        "      {s: sum(1 for _ in open(f\"{DATA_DIR}/{s}.csv\"))-1 for s in [\"train\",\"val\",\"test\"]})"
      ],
      "metadata": {
        "id": "axF_4s0sdmyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- config ---\n",
        "LABELS = [\"normal\",\"abnormal\",\"uncertain\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "MAX_LEN = 128\n",
        "BATCH_TRAIN = 32\n",
        "BATCH_EVAL  = 64\n",
        "LR = 2e-5\n",
        "EPOCHS = 3\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "cDPmrZxZSSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, os\n",
        "def need(path):\n",
        "    assert os.path.exists(path), f\"Missing {path}\"\n",
        "    df = pd.read_csv(path)\n",
        "    req = {\"report_id\",\"sentence_id\",\"text\",\"label\"}\n",
        "    assert req.issubset(df.columns), f\"{path} must have columns {req}\"\n",
        "    return df\n",
        "\n",
        "train_df = need(f\"{DATA_DIR}/train.csv\")\n",
        "val_df   = need(f\"{DATA_DIR}/val.csv\")\n",
        "test_df  = need(f\"{DATA_DIR}/test.csv\")\n",
        "\n",
        "for d in (train_df, val_df, test_df):\n",
        "    d[\"labels\"] = d[\"label\"].map(label2id)"
      ],
      "metadata": {
        "id": "D1kb_0ENSWUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def to_ds(df): return Dataset.from_pandas(df[[\"text\",\"labels\"]].reset_index(drop=True))\n",
        "train_ds, val_ds, test_ds = map(to_ds, [train_df, val_df, test_df])\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "train_tok = train_ds.map(tokenize, batched=True).with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
        "val_tok   = val_ds.map(tokenize,   batched=True).with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
        "test_tok  = test_ds.map(tokenize,  batched=True).with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])"
      ],
      "metadata": {
        "id": "9XmN2tuVTFvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, evaluate\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "metric_f1  = evaluate.load(\"f1\")\n",
        "metric_auc = evaluate.load(\"roc_auc\",\"multiclass\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    out = {\n",
        "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
        "    }\n",
        "    try:\n",
        "        out[\"auc_ovr\"] = metric_auc.compute(\n",
        "            prediction_scores=logits, references=labels, multi_class=\"ovr\", average=\"macro\"\n",
        "        )[\"roc_auc\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out"
      ],
      "metadata": {
        "id": "8e-YJ3cBT-Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, time, json\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, set_seed\n",
        "import os\n",
        "set_seed(SEED)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=3, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "run_name = f\"ce_no_weights_{MODEL_NAME.split('/')[-1]}_{int(time.time())}\"\n",
        "out_dir  = f\"{CKPT_DIR}/{run_name}\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    per_device_train_batch_size=BATCH_TRAIN,\n",
        "    per_device_eval_batch_size=BATCH_EVAL,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    save_safetensors=False,\n",
        "    overwrite_output_dir=True,\n",
        ")\n",
        "\n",
        "# REMOVING WEIGHTED TRAINER\n",
        "\n",
        "\"\"\"\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\"\"\"\n",
        "#reglar trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "vrpj9LyeUEMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_out = trainer.train()\n",
        "test_metrics = trainer.evaluate(test_tok)\n",
        "test_metrics"
      ],
      "metadata": {
        "id": "kMEcGlJiUPSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "os.makedirs(FIGS_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(out_dir, \"test_metrics.json\"), \"w\") as f:\n",
        "    json.dump(test_metrics, f, indent=2)\n",
        "pd.DataFrame([test_metrics]).to_csv(os.path.join(out_dir, \"test_metrics.csv\"), index=False)\n",
        "\n",
        "keys = [k for k in [\"eval_accuracy\",\"eval_f1_macro\"] if k in test_metrics]\n",
        "vals = [test_metrics[k] for k in keys]\n",
        "plt.figure(); plt.bar(keys, vals)\n",
        "for i,v in enumerate(vals): plt.text(i, v, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
        "plt.title(\"Bio_ClinicalBERT (no class weights) â€” Test\")\n",
        "fig_path = os.path.join(FIGS_DIR, f\"{os.path.basename(out_dir)}_metrics.png\")\n",
        "plt.savefig(fig_path, bbox_inches=\"tight\"); plt.show()\n",
        "\n",
        "{\"out_dir\": out_dir, \"fig\": fig_path}"
      ],
      "metadata": {
        "id": "-31R21I-aDgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "preds = trainer.predict(test_tok)\n",
        "y_true = preds.label_ids\n",
        "y_pred = np.argmax(preds.predictions, axis=-1)\n",
        "\n",
        "LABELS = [\"normal\",\"abnormal\",\"uncertain\"]\n",
        "print(classification_report(y_true, y_pred, target_names=LABELS, digits=3))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=range(len(LABELS)))\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in LABELS],\n",
        "                         columns=[f\"pred_{l}\" for l in LABELS])\n",
        "cm_df"
      ],
      "metadata": {
        "id": "2OjyzbUHsb9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "train_epochs, train_losses = [], []\n",
        "val_epochs,   val_losses   = [], []\n",
        "\n",
        "for rec in logs:\n",
        "    if \"loss\" in rec and \"epoch\" in rec and \"eval_loss\" not in rec:\n",
        "        train_epochs.append(rec[\"epoch\"])\n",
        "        train_losses.append(rec[\"loss\"])\n",
        "    if \"eval_loss\" in rec and \"epoch\" in rec:\n",
        "        val_epochs.append(rec[\"epoch\"])\n",
        "        val_losses.append(rec[\"eval_loss\"])\n",
        "\n",
        "print(\"Train loss points:\", len(train_losses))\n",
        "print(\"Val loss points:\", len(val_losses))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_epochs, train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_epochs,   val_losses,   label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "loss_fig_path = os.path.join(FIGS_DIR, \"baseline_loss_curve.png\")\n",
        "plt.savefig(loss_fig_path, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "loss_fig_path"
      ],
      "metadata": {
        "id": "rVNCokb-3tvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}